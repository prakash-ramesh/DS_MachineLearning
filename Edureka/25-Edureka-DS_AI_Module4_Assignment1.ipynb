{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeCorpus(str_list):\n",
    "    corpus = set()\n",
    "\n",
    "    for str in str_list:\n",
    "        tokens = word_tokenize(str)\n",
    "        \n",
    "        for word in tokens:\n",
    "            corpus.add(word)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India', 'final', 'won', 'Australia', 'the', 'match', 'England', 'cricket']\n"
     ]
    }
   ],
   "source": [
    "s1= \"India won the match\"\n",
    "s2= \"England won the cricket match\"\n",
    "s3= \"Australia won the final match\"\n",
    "\n",
    "lst = [s1,s2,s3]\n",
    "\n",
    "corpus = MakeCorpus(lst)\n",
    "print(list(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PresenceAbsenceVectorization(str_list):\n",
    "    str_list = [s.lower() for s in str_list]\n",
    "\n",
    "    print(str_list)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    vectors = vectorizer.fit_transform(str_list)\n",
    "\n",
    "    return vectors.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['india won the match', 'england won the cricket match', 'australia won the final match']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 0, 0, 1, 1, 1], [1, 0, 0, 1, 0, 1, 1, 1]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PresenceAbsenceVectorization(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVectorization(str_list):\n",
    "    str_list = [s.replace('.', '').lower() for s in str_list]\n",
    "    \n",
    "    vectorizer = CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "\n",
    "    vectors = vectorizer.fit_transform(str_list)\n",
    "\n",
    "    return vectors.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 1, 1, 0, 1, 1, 2], [0, 2, 1, 1, 1, 1, 2], [1, 0, 2, 1, 1, 1, 2]]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = ['A lives with B. A plays with C', 'B lives with C. B plays with D', 'C lives with D. C plays with A']\n",
    "CountVectorization(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TFIDFVectorization(str_list):\n",
    "    str_list = [s.replace('.', '').lower() for s in str_list]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(tokenizer=lambda txt: txt.split())\n",
    "\n",
    "    vectors = vectorizer.fit_transform(str_list)\n",
    "\n",
    "    print(vectorizer.get_feature_names_out())\n",
    "    return vectors.toarray().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a' 'b' 'c' 'd' 'lives' 'plays' 'with']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.6586065078781591,\n",
       "  0.32930325393907955,\n",
       "  0.25573335296349775,\n",
       "  0.0,\n",
       "  0.25573335296349775,\n",
       "  0.25573335296349775,\n",
       "  0.5114667059269955],\n",
       " [0.0,\n",
       "  0.6586065078781591,\n",
       "  0.25573335296349775,\n",
       "  0.32930325393907955,\n",
       "  0.25573335296349775,\n",
       "  0.25573335296349775,\n",
       "  0.5114667059269955],\n",
       " [0.35287238714703795,\n",
       "  0.0,\n",
       "  0.5480737748800985,\n",
       "  0.35287238714703795,\n",
       "  0.2740368874400492,\n",
       "  0.2740368874400492,\n",
       "  0.5480737748800985]]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFVectorization(lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
